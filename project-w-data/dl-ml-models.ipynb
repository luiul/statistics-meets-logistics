{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics Meets Logistics\n",
    "--- \n",
    "\n",
    "## Description\n",
    "This notebook holds the DataFrames and analysis for the project. Requirements for the project environment can be found in https://github.com/luiul/statistics-meets-logistics/blob/main/requirements.txt. **Disclaimer**: the project has not been tested in other environments. \n",
    "\n",
    "## Goal \n",
    "The goal of this project is to perform a regression analysis given raw download and upload data to estimate the throughput of the system, i.e. the label we're trying to predict. \n",
    "\n",
    "## Overview\n",
    "We were given raw download and upload data collected from ...\n",
    "\n",
    "## In General\n",
    "In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome variable') and one or more independent variables (often called 'predictors', 'covariates', or 'features').\n",
    "\n",
    "## Question\n",
    "What is the predicted throughput?\n",
    "\n",
    "## Architecture Model Described in the Paper\n",
    "<img src=\"./figures/architecture.png\" width=\"600\" alt=\"Architecture model for the client-based data rate prediction.\" class=\"center\">\n",
    "\n",
    "\n",
    "\n",
    "## From the Article: Boosting VtC Communication by ML-enabled Context Prediction\n",
    "\n",
    "Article propose a client-side opportunistic transmission scheme that applies machine learning-based data rate prediction for scheduling the transmission times of sensor data transmissions with respect to the expected resource-efficiency\n",
    "\n",
    "The studies agree that passively measurable network quality indicators such as Reference Signal Received Power (RSRP), Reference Signal Received Quality (RSRQ), Signal- to-interference-plus-noise Ratio (SINR), and Channel Quality Indicator (CQI) provide meaningful information, which can be leveraged to estimate the resulting data rate based on machine learning methods even in challenging environments. In comparison to time series-based active data rate prediction (e.g., based on Kalman filters), passive approaches do not monitor the data rates of ongoing transmissions and can therefore be applied without introducing additional traffic themselves. As resource efficiency is one of the optimization goals of this work, we focus on passive data rate prediction.\n",
    "\n",
    "In this context, the usage of connectivity maps for anticipatory communication allows to exploit a priori information about the channel quality based on previous measurements in the same geographical area. Radio Environment Maps (REMs) implement a similar concept, which enables opportunistic data transfer with Cognitive Radio (CR) methods. However, those purely spectrum-aware approaches do not consider the cross-layer interdependencies within the protocol stack. Moreover, as the resource allocation in LTE is performed by the scheduling mechanisms of the evolved NodeB (eNB), those methods have to be imple- mented by the mobile network operator. In contrast to that, the proposed machine learning-based approach can easily be implemented on the client side without requiring modifications to the network infrastructure.\n",
    "\n",
    "The feature set of the data rate prediction is composed of the network quality indicators, the velocity and the payload size of the data packet. The resulting data rate of the active transmis- sion is used as the label for the prediction process, which is performed with the models Artificial Neural Network (ANN), Linear Regression (LR), Random Forest (RF), M5 Decision Tree (M5T) and Support Vector Machine (SVM). Finally, the prediction performance of the different models is evaluated using 10-fold cross validation. Additionally, the measured channel context parameters and the position information of the vehicle are utilized to create a multi-layer connectivity map that stores the cell-wise average of each indicator from multiple visits of the same geographical area.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Set Options\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector (Series) & Matrix (DateFrame) manipulation \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "# If JaveScript is configured and enabled: \n",
    "# static images: \n",
    "# %matplotlib inline\n",
    "\n",
    "# interactive images: \n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Data Visualization\n",
    "# import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Utilities\n",
    "# Generate datetime objects from raw timestamps and vice versa\n",
    "from datetime import datetime\n",
    "\n",
    "# OS Interface\n",
    "# import os\n",
    "\n",
    "# Regex search patterns \n",
    "# import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling np.version.version should return 1.18.1\n",
    "# np.version.version\n",
    "\n",
    "# calling pd.__version__ should return 1.1.2\n",
    "# pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)\n",
    "# avoid truncate view of DataFrame (scroll to view all columns); set to 0 for pandas to auto-detect the with of the terminal and print truncated object that fits the screen width\n",
    "\n",
    "# pd.set_option('float_format', '{:.2f}'.format)\n",
    "# prints floats with two decimal points; do not comment out in this project since the features lat and lon have sigficant figures after two decimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all outsputs if the cell has multiple commands as its input\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train | Test Split & Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Function (see Signature for correct tuple unpacking)\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Default split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When perfroming a classic Train | Test Spit fit ONLY to X_train to avoid data leakage! (Follow Procedure described in documentation under Cross Validation and Linear Regression Project)\n",
    "\n",
    "# Data Scaling (iff values are in different order of magnitude)\n",
    "# from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross validation scores; estimator = ML model, cv = fold value, scoring = error metric (use the ones provided by sklearn!)\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Regression ( poly_trafo: X->X*...*X )\n",
    "# from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search with cross vadlidation\n",
    "# from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "# from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net Regularization: start here for regularization in Linear Regression. Make sure to keep an l1_ratio that allows us to go fully to Lasso or fully to Ridge. See Lasso and Ridge explanations below. \n",
    "# from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# Use from sklearn.linear_model import ElasticNet in case CV done manually / grid search\n",
    "\n",
    "# Standard procedure with no grid search: create X and y, split data, scale data (stadardize)\n",
    "# Standard procedure with grid search: create X and y, split data, scale data (stadardize), instantiate base model, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2: Ridge Regularization: adds beta squared shrinkage penalty. Hyper-parameter alpha: alpha=0 -> RSS minimization. L2 CV takes an alpha tuple and computes the hyper-parameter that delivers the best performance (either based on default scorer or one from the SCORES dictionary)\n",
    "# from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Use from sklearn.linear_model import Ridge in case CV done manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1: Lasso Regularization: adds absolute beta value shrinkage penalty. Hyper-parameter alpha: alpha=0 -> RSS minimization. There are two ways to determine the alpha hyper-parameter: (a) provide list of alphas as an array (b) alpha can be set automatically by the class based off epsilon and n_alphas (we use the default values)\n",
    "# from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Use from sklearn.linear_model import Lasso in case no CV done manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVR\n",
    "\n",
    "# from sklearn.svm import LinearSVC\n",
    "# faster than the generic version with the caveat that it only has a linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Evaluation: common evaluation metrics; they can also be found in the SCORES dictionary (although transformed s.t. the higher the score the better, the model performance)\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with different scorer objects; higher return values are better than lower return values by convention, e.g. negative error maximization -> the higher the score, the better the model performance\n",
    "# from sklearn.metrics import SCORERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Probability Plot\n",
    "# import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Model Deployment \n",
    "# from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
